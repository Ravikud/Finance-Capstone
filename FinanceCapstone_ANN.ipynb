{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinanceCapstone",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho3z1I1BeEXn",
        "outputId": "b22a59e2-bf4b-493a-970d-060e8dc44309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-ijad4zeQOX"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8D7Oxs1ez9B"
      },
      "source": [
        "dftrain = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Financial/train_data.csv\")\n",
        "dftest = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/Financial/test_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl4PU8KEfGnk"
      },
      "source": [
        "dftrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo79znTSewks",
        "outputId": "7e41978d-c4a3-42ae-eff5-0c28ec173c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "dftrain.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(227845, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9uZ3K6MgJtG",
        "outputId": "f935a79a-f907-4c20-a570-4432beca542d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "#Apply Standard Scaling for features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "dftrain['Amount'] = StandardScaler().fit_transform(dftrain['Amount'].values.reshape(-1,1))\n",
        "dftrain.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>38355.0</td>\n",
              "      <td>1.043949</td>\n",
              "      <td>0.318555</td>\n",
              "      <td>1.045810</td>\n",
              "      <td>2.805989</td>\n",
              "      <td>-0.561113</td>\n",
              "      <td>-0.367956</td>\n",
              "      <td>0.032736</td>\n",
              "      <td>-0.042333</td>\n",
              "      <td>-0.322674</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>-0.572665</td>\n",
              "      <td>0.346009</td>\n",
              "      <td>-0.047407</td>\n",
              "      <td>-0.098964</td>\n",
              "      <td>-0.663284</td>\n",
              "      <td>0.181411</td>\n",
              "      <td>-0.124345</td>\n",
              "      <td>-0.790453</td>\n",
              "      <td>-0.720944</td>\n",
              "      <td>-0.084556</td>\n",
              "      <td>-0.240105</td>\n",
              "      <td>-0.680315</td>\n",
              "      <td>0.085328</td>\n",
              "      <td>0.684812</td>\n",
              "      <td>0.318620</td>\n",
              "      <td>-0.204963</td>\n",
              "      <td>0.001662</td>\n",
              "      <td>0.037894</td>\n",
              "      <td>-0.156600</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>22555.0</td>\n",
              "      <td>-1.665159</td>\n",
              "      <td>0.808440</td>\n",
              "      <td>1.805627</td>\n",
              "      <td>1.903416</td>\n",
              "      <td>-0.821627</td>\n",
              "      <td>0.934790</td>\n",
              "      <td>-0.824802</td>\n",
              "      <td>0.975890</td>\n",
              "      <td>1.747469</td>\n",
              "      <td>-0.658751</td>\n",
              "      <td>1.281502</td>\n",
              "      <td>-1.430087</td>\n",
              "      <td>0.372028</td>\n",
              "      <td>1.403024</td>\n",
              "      <td>-2.739413</td>\n",
              "      <td>-1.331766</td>\n",
              "      <td>1.964590</td>\n",
              "      <td>-0.205639</td>\n",
              "      <td>1.325588</td>\n",
              "      <td>-0.373759</td>\n",
              "      <td>-0.335332</td>\n",
              "      <td>-0.510994</td>\n",
              "      <td>0.035839</td>\n",
              "      <td>0.147565</td>\n",
              "      <td>-0.529358</td>\n",
              "      <td>-0.566950</td>\n",
              "      <td>-0.595998</td>\n",
              "      <td>-0.220086</td>\n",
              "      <td>-0.288523</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2431.0</td>\n",
              "      <td>-0.324096</td>\n",
              "      <td>0.601836</td>\n",
              "      <td>0.865329</td>\n",
              "      <td>-2.138000</td>\n",
              "      <td>0.294663</td>\n",
              "      <td>-1.251553</td>\n",
              "      <td>1.072114</td>\n",
              "      <td>-0.334896</td>\n",
              "      <td>1.071268</td>\n",
              "      <td>-1.109522</td>\n",
              "      <td>-1.016020</td>\n",
              "      <td>-0.654945</td>\n",
              "      <td>-1.473470</td>\n",
              "      <td>0.317345</td>\n",
              "      <td>1.067491</td>\n",
              "      <td>-0.372642</td>\n",
              "      <td>-0.674725</td>\n",
              "      <td>0.369841</td>\n",
              "      <td>0.095583</td>\n",
              "      <td>-0.039868</td>\n",
              "      <td>0.012220</td>\n",
              "      <td>0.352856</td>\n",
              "      <td>-0.341505</td>\n",
              "      <td>-0.145791</td>\n",
              "      <td>0.094194</td>\n",
              "      <td>-0.804026</td>\n",
              "      <td>0.229428</td>\n",
              "      <td>-0.021623</td>\n",
              "      <td>-0.352771</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>86773.0</td>\n",
              "      <td>-0.258270</td>\n",
              "      <td>1.217501</td>\n",
              "      <td>-0.585348</td>\n",
              "      <td>-0.875347</td>\n",
              "      <td>1.222481</td>\n",
              "      <td>-0.311027</td>\n",
              "      <td>1.073860</td>\n",
              "      <td>-0.161408</td>\n",
              "      <td>0.200665</td>\n",
              "      <td>0.154307</td>\n",
              "      <td>0.882673</td>\n",
              "      <td>0.547890</td>\n",
              "      <td>0.269484</td>\n",
              "      <td>-1.253302</td>\n",
              "      <td>-0.883963</td>\n",
              "      <td>0.495221</td>\n",
              "      <td>-0.153212</td>\n",
              "      <td>0.296710</td>\n",
              "      <td>0.136148</td>\n",
              "      <td>0.382305</td>\n",
              "      <td>-0.424626</td>\n",
              "      <td>-0.781158</td>\n",
              "      <td>0.019316</td>\n",
              "      <td>0.178614</td>\n",
              "      <td>-0.315616</td>\n",
              "      <td>0.096665</td>\n",
              "      <td>0.269740</td>\n",
              "      <td>-0.020635</td>\n",
              "      <td>-0.313351</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>127202.0</td>\n",
              "      <td>2.142162</td>\n",
              "      <td>-0.494988</td>\n",
              "      <td>-1.936511</td>\n",
              "      <td>-0.818288</td>\n",
              "      <td>-0.025213</td>\n",
              "      <td>-1.027245</td>\n",
              "      <td>-0.151627</td>\n",
              "      <td>-0.305750</td>\n",
              "      <td>-0.869482</td>\n",
              "      <td>0.428729</td>\n",
              "      <td>1.136666</td>\n",
              "      <td>0.273476</td>\n",
              "      <td>0.697123</td>\n",
              "      <td>-1.222134</td>\n",
              "      <td>-0.938820</td>\n",
              "      <td>1.298149</td>\n",
              "      <td>0.912921</td>\n",
              "      <td>-0.793721</td>\n",
              "      <td>1.064984</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.010115</td>\n",
              "      <td>0.021722</td>\n",
              "      <td>0.079463</td>\n",
              "      <td>-0.480899</td>\n",
              "      <td>0.023846</td>\n",
              "      <td>-0.279076</td>\n",
              "      <td>-0.030121</td>\n",
              "      <td>-0.043888</td>\n",
              "      <td>-0.195737</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Time        V1        V2        V3  ...       V27       V28    Amount  Class\n",
              "0   38355.0  1.043949  0.318555  1.045810  ...  0.001662  0.037894 -0.156600      0\n",
              "1   22555.0 -1.665159  0.808440  1.805627  ... -0.595998 -0.220086 -0.288523      0\n",
              "2    2431.0 -0.324096  0.601836  0.865329  ...  0.229428 -0.021623 -0.352771      0\n",
              "3   86773.0 -0.258270  1.217501 -0.585348  ...  0.269740 -0.020635 -0.313351      0\n",
              "4  127202.0  2.142162 -0.494988 -1.936511  ... -0.030121 -0.043888 -0.195737      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwFvZ9Pjg5Eo"
      },
      "source": [
        "#Drop the columns which are unnecessary\n",
        "dftrain = dftrain.drop(columns=['Time'],axis=1)\n",
        "#Independent Features\n",
        "X = dftrain.drop(['Class'],axis=1)\n",
        "#Dependent Features\n",
        "y = dftrain['Class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2GB-MfxhX1B"
      },
      "source": [
        "#Split the training and testing dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvjVb53bZ5DA"
      },
      "source": [
        "# We are transforming data to numpy array to implementing with keras\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0f-eryWUhyq"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "#Apply Sequential model\n",
        "model = Sequential([\n",
        "    #Apply Input layer of 18 neurons  activation function relu\n",
        "    Dense(units=18, input_dim = 29,activation='relu'),\n",
        "    #Apply hidden layer of 26 neurons  activation function relu\n",
        "    Dense(units=26,activation='relu'),\n",
        "    #Apply Dropout layer\n",
        "    Dropout(0.5),\n",
        "   # Dense(22,activation='relu'),\n",
        "    #Dense(22,activation='relu'),\n",
        "    #Apply output layer of activation function sigmoid\n",
        "    Dense(1,activation='sigmoid'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSgYl3vlGrx7",
        "outputId": "cb7aafb2-faa0-44f6-9c08-537c58b63f08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 18)                540       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 26)                494       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 26)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 27        \n",
            "=================================================================\n",
            "Total params: 1,061\n",
            "Trainable params: 1,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJiE0HuSGyUY",
        "outputId": "101c43dc-9712-4a7e-c383-880f91345fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.fit(X_train,y_train,batch_size=10,epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "15950/15950 [==============================] - 41s 3ms/step - loss: 0.0123 - accuracy: 0.9976\n",
            "Epoch 2/10\n",
            "15950/15950 [==============================] - 40s 3ms/step - loss: 0.0048 - accuracy: 0.9992\n",
            "Epoch 3/10\n",
            "15950/15950 [==============================] - 40s 3ms/step - loss: 0.0041 - accuracy: 0.9993\n",
            "Epoch 4/10\n",
            "15950/15950 [==============================] - 40s 3ms/step - loss: 0.0041 - accuracy: 0.9993\n",
            "Epoch 5/10\n",
            "15950/15950 [==============================] - 41s 3ms/step - loss: 0.0036 - accuracy: 0.9992\n",
            "Epoch 6/10\n",
            "15950/15950 [==============================] - 41s 3ms/step - loss: 0.0036 - accuracy: 0.9993\n",
            "Epoch 7/10\n",
            "15950/15950 [==============================] - 40s 3ms/step - loss: 0.0034 - accuracy: 0.9993\n",
            "Epoch 8/10\n",
            "15950/15950 [==============================] - 40s 3ms/step - loss: 0.0032 - accuracy: 0.9993\n",
            "Epoch 9/10\n",
            "15950/15950 [==============================] - 40s 3ms/step - loss: 0.0031 - accuracy: 0.9994\n",
            "Epoch 10/10\n",
            "15950/15950 [==============================] - 40s 3ms/step - loss: 0.0032 - accuracy: 0.9994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc3902c97b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROWQXy7sGxWP",
        "outputId": "b993c628-0ebd-4599-e45a-821ecbfe6bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test Accuracy: {:.2f}%\\nTest Loss: {}'.format(score[1]*100,score[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2137/2137 [==============================] - 4s 2ms/step - loss: 0.0031 - accuracy: 0.9996\n",
            "Test Accuracy: 99.96%\n",
            "Test Loss: 0.0030737726483494043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWyp4q4FaoU-",
        "outputId": "01a14570-8ef3-480a-f1f5-f0385d6a0383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = model.predict(X_test)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "cm = confusion_matrix(y_test, y_pred.round())\n",
        "sns.heatmap(cm, annot=True, fmt='.0f')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD4CAYAAAAn3bdmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYbUlEQVR4nO3de5SVxZnv8e8PlEicUUCUe8QL0cHE+wU1Jo4aBE2Cyck4miwhhmNnIhp1MhE1UY63SLxEZKJGFBS8EeKJS2JUQtCZxJNBwXhFxthDwtg9XFRQoyRCdz/njy5wB5vdu6W7d9fr78Oq1e/7VL3vrr1Wr6eLqnr3VkRgZmZ56FbtDpiZWeWctM3MMuKkbWaWESdtM7OMOGmbmWVkm45+gQ2vLfP2FHufngOPqnYXrAtqWF+vrb1HW3LOtn133+rX62weaZuZZaTDR9pmZp2qqbHaPehQTtpmViyNDdXuQYdy0jazQoloqnYXOpSTtpkVS5OTtplZPjzSNjPLiBcizcwy4pG2mVk+wrtHzMwy4oVIM7OMeHrEzCwjXog0M8uIR9pmZhnxQqSZWUa8EGlmlo8Iz2mbmeXDc9pmZhnx9IiZWUY80jYzy0jjhmr3oEM5aZtZsXh6xMwsIwWfHvG3sZtZsTQ1VV5aIamXpPsk/aekpZIOl9RH0nxJL6efvVNbSZoqqVbSc5IOLLnPuNT+ZUnjSuIHSXo+XTNVklrrk5O2mRVLOyZt4AbgkYjYG9gPWApcACyIiGHAgnQOMBoYlkoNcDOApD7AJOAw4FBg0sZEn9qcUXLdqNY65KRtZoUSjRsqLuVI2hH4NDAdICLWR8QbwBhgZmo2EzgpHY8BZkWzhUAvSQOA44H5EbEmItYC84FRqW6HiFgYEQHMKrnXFjlpm1mxRFPlpbzdgFeB2yU9Lek2SdsD/SJiRWqzEuiXjgcBr5RcX5di5eJ1LcTLctI2s2Jpw/SIpBpJi0tKTcmdtgEOBG6OiAOAd3hvKgSANEKOzntz3j1iZkXTht0jETENmLaF6jqgLiKeSOf30Zy0V0kaEBEr0hTH6lRfDwwpuX5witUDR28W/7cUH9xC+7I80jazYmmnhciIWAm8ImmvFDoWeBGYC2zcATIOeCAdzwXGpl0kI4A30zTKPGCkpN5pAXIkMC/VvSVpRNo1MrbkXlvkkbaZFUv77tM+G7hbUg9gGXA6zYPdOZLGA8uBk1Pbh4ATgFpgXWpLRKyRdDmwKLW7LCLWpOMzgTuAnsDDqZTlpG1mxdLQfl+CEBHPAAe3UHVsC20DmLCF+8wAZrQQXwx8oi19ctI2s2Ip+BORTtpmViz+7BEzs4x4pG1mlhGPtM3MMuKRtplZRtpx90hX5KRtZsUSnfpUeadz0jazYvGctplZRpy0zcwy4oVIM7OMNDZWuwcdyknbzIrF0yNmZhlx0jYzy4jntM3M8hFN3qdtZpYPT4+YmWXEu0fMzDLikbaZWUactG1zb/3pbSZNnkLtsuUgcflF57Fdjx5cds2/8u76DXTv3p2L/2UCnxy+Fw/Oe5Tpd/8UAj760Z5c/C9nsfew3Vmx6lUuuvxaXl+7FiG+PGY0p518EgDzHv0NN02/i2XLX+HeW6fwib/7eJXfsbWXW6ddx4knHMfqV19j/wOav2bwkov/mfFf/wqvvtb8Xa8XXzyZhx95tJrdzJs/MMo2N3nKjznysIO5/srvsWHDBv78l3f59sXf55tf/ypHHX4Iv/7tk1x303Tu+NHVDBrYnzt+dDU77vC3/OY/FnHp1VO599YpbNO9O985+wyG77Un77yzjpPHf4sjDjmAPXbblT1335Up37+YS6+ZWu23au1s1qw53HTT7dx++w1/Fb9h6q388PpbqtSrgvmwj7Ql7Q2MAQalUD0wNyKWdmTHuqo/vf0OTz37Ald+79sAbLvttmy77bZI4u131gHw9jvr2KXvTgAc8Mnhm67dd5+9WbX6NQB27tuHnfv2AWD77T/K7rsOYdWrr7PHbruyx9CPdeZbsk70m8efYNddB1e7G8VW8C1/3cpVSpoIzAYEPJmKgHslXdDx3et66v9nJb177cj3rvwhX/7aBC65agrr/vwXJp7zDa67aTrHfvE0rv3RbZz7T19737U/e3Aenxpx8PvvuWIVS1/+L/bdZ69OeAfWFZ35zdP53VPzuXXadfTqtWO1u5O3xsbKS4bKJm1gPHBIREyOiLtSmQwcmupaJKlG0mJJi2+bdW979rfqGhobWfr7Wv7xiydy3x030rPndky/cw4/uf8XTDy7hgX338n536rhkqum/NV1Tz71LD978Jf885lf/6v4unV/5rzvXsHEb32Dv9l++858K9ZF/PiWWXx87yM46OCRrFy5mmuuvqTaXcpaNDVVXFoj6Y+Snpf0jKTFKdZH0nxJL6efvVNckqZKqpX0nKQDS+4zLrV/WdK4kvhB6f616Vq11qfWknYTMLCF+IBU16KImBYRB0fEwf977Kmt9SEr/XfpS7+d+7LvPnsDMPLoT/Hi72uZ+/CvOO7oIwE4/pijeP7FlzZd81LtH7hk8hT+dfIl9Npxh03xDQ0NnPvdKzhx5N/z2XStffisXv0aTU1NRAS3Tb+bQw7Zv9pdyltTVF4q8/cRsX9EbPxv8gXAgogYBixI5wCjgWGp1AA3Q3OSByYBh9E84J20MdGnNmeUXDeqtc60lrTPBRZIeljStFQeSR09p5J3WzR9d+pD/1125g/L6wBY+NQz7DH0Y+zcdycWPf08AE889Qy7DmleAlixcjXnXnQ5V13yHYZ+7L25zIjgkqumsPuuQxh3ypc6/41Yl9G//y6bjk8aM5olS14q09paFU2Vlw9mDDAzHc8ETiqJz4pmC4FekgYAxwPzI2JNRKwF5gOjUt0OEbEwIgKYVXKvLSq7EBkRj0j6OM1/HUoXIhdFRJ4TQu3govO+ycRLr2ZDwwaGDBzA5RedxzFHjWDyDbfQ0NjIR3r0YNL53wLg5tvv4c23/sQV194IQPfu3ZkzYypPP7eEnz+ygGF7DOV/jZsAwDnfGMenjziUX/37/+Oq629mzRtvcuZ3JrH3sN2Zdv2VVXu/1n7uuvNGPvPpw+nbtw9/XLaYSy+7ls985gj22284EcHy5XV888yJ1e5m3tqwECmphuZR8UbTImJayXkAv5QUwC2prl9ErEj1K4F+6XgQ8ErJtXUpVi5e10K8fJ+jg/c0bnhtWbGXcu0D6TnwqGp3wbqghvX1rc7ptuadS06pOOdsf9nssq8naVBE1EvaheYR8tk0757rVdJmbUT0lvQgMDkiHk/xBcBE4Ghgu4i4IsUvBv4M/Ftqf1yKHwVMjIjPletTa9MjZmZ5acfpkYioTz9XA/fTPOuwKk1tkH6uTs3rgSEllw9OsXLxwS3Ey3LSNrNiaaeFSEnbS/rbjcfASOAFYC6wcQfIOOCBdDwXGJt2kYwA3kzTKPOAkZJ6pwXIkcC8VPeWpBFp18jYknttkZ+INLNCqWQrX4X6AfenXXjbAPekdb5FwBxJ44HlwMmp/UPACUAtsA44HSAi1ki6HFiU2l0WEWvS8ZnAHUBP4OFUynLSNrNiaacnIiNiGbBfC/HXgWNbiAcwYQv3mgHMaCG+GPhEW/rlpG1mxVLwx9idtM2sWDJ9PL1STtpmVij+jkgzs5w4aZuZZeTD/nnaZmZZ8UjbzCwjTtpmZvmIRk+PmJnlwyNtM7N8eMufmVlOnLTNzDJS7CltJ20zK5ZoKHbWdtI2s2Ipds520jazYvFCpJlZTjzSNjPLh0faZmY58UjbzCwf0VDtHnQsJ20zK5TwSNvMLCNO2mZm+fBI28wsI0VP2t2q3QEzs/YUjaq4VEJSd0lPS3owne8m6QlJtZJ+IqlHin8kndem+qEl97gwxV+SdHxJfFSK1Uq6oJL+OGmbWaFEU+WlQucAS0vOfwBcHxF7AmuB8Sk+Hlib4tendkgaDpwC7AOMAm5Kfwi6AzcCo4HhwKmpbVlO2mZWKNGkiktrJA0GTgRuS+cCjgHuS01mAiel4zHpnFR/bGo/BpgdEe9GxB+AWuDQVGojYllErAdmp7ZlOWmbWaG0ZaQtqUbS4pJSs9ntpgDn896elJ2ANyI27QavAwal40HAKwCp/s3UflN8s2u2FC/LC5FmVigRlc1VN7eNacC0luokfQ5YHRFPSTq6fXq39Zy0zaxQ2nH3yJHAFySdAGwH7ADcAPSStE0aTQ8G6lP7emAIUCdpG2BH4PWS+Eal12wpvkWeHjGzQmlqVMWlnIi4MCIGR8RQmhcSH42IrwKPAV9OzcYBD6TjuemcVP9oRESKn5J2l+wGDAOeBBYBw9JulB7pNea29v480jazQqlkgXErTQRmS7oCeBqYnuLTgTsl1QJraE7CRMQSSXOAF4EGYEJENAJIOguYB3QHZkTEktZeXM1/CDrOhteWFftzEu0D6TnwqGp3wbqghvX1W51x/7j/ZyvOOUOfmd/hGb69eaRtZoXSwePQqnPSNrNC6YTpkapy0jazQmnLlr8cOWmbWaE0VviZIrly0jazQvFI28wsI57TNjPLiHePmJllxCNtM7OMNDYV+9M5nLTNrFA8PWJmlpEm7x4xM8uHt/yZmWXE0yNbyZ/mZmadydMjZmYZ8e4RM7OMFHx2xEnbzIrF0yNmZhnx7hEzs4y035exd01O2mZWKIFH2mZm2Wjw9IiZWT480jYzy0jR57SLvQvdzD50AlVcypG0naQnJT0raYmkS1N8N0lPSKqV9BNJPVL8I+m8NtUPLbnXhSn+kqTjS+KjUqxW0gWVvD8nbTMrlKY2lFa8CxwTEfsB+wOjJI0AfgBcHxF7AmuB8an9eGBtil+f2iFpOHAKsA8wCrhJUndJ3YEbgdHAcODU1LYsJ20zK5RGVHEpJ5q9nU63TSWAY4D7UnwmcFI6HpPOSfXHSlKKz46IdyPiD0AtcGgqtRGxLCLWA7NT27KctM2sUJpUeZFUI2lxSakpvVcaET8DrAbmA/8FvBERDalJHTAoHQ8CXgFI9W8CO5XGN7tmS/GyvBBpZoXS1IbdIxExDZhWpr4R2F9SL+B+YO+t7uBW8kjbzAol2lAqvmfEG8BjwOFAL0kbB7yDgfp0XA8MAUj1OwKvl8Y3u2ZL8bKctM2sUNprIVLSzmmEjaSewGeBpTQn7y+nZuOAB9Lx3HROqn80IiLFT0m7S3YDhgFPAouAYWk3Sg+aFyvntvb+PD1iZoXSpHZ7uGYAMDPt8ugGzImIByW9CMyWdAXwNDA9tZ8O3CmpFlhDcxImIpZImgO8CDQAE9K0C5LOAuYB3YEZEbGktU4pOvi7ebbpMajoH29rZu2kYX39Vmfcnwz4asU55x9X3J3d45MeaZtZoTRll4bbxknbzAqlLbtHcuSkbWaFUvT5WCdtMysUT4+YmWWk6J/y56RtZoXS6JG2mVk+PNI2M8uIk7aZWUYK/hWRTtpmViweaZuZZaSx2h3oYE7aZlYo3qdtZpYRT4+YmWXESdvMLCP+7BEzs4x4TtvMLCPePWJmlpGmgk+QOGmbWaF4IdLMLCPFHmc7aZtZwXikbWaWkQYVe6ztpG1mhVLslA3dqt0BM7P21NSGUo6kIZIek/SipCWSzknxPpLmS3o5/eyd4pI0VVKtpOckHVhyr3Gp/cuSxpXED5L0fLpmqqRWd5k7aZtZoTQRFZdWNADfjojhwAhggqThwAXAgogYBixI5wCjgWGp1AA3Q3OSByYBhwGHApM2JvrU5oyS60a11iknbTMrlGhDKXufiBUR8bt0/CdgKTAIGAPMTM1mAiel4zHArGi2EOglaQBwPDA/ItZExFpgPjAq1e0QEQsjIoBZJffaIidtMyuUtkyPSKqRtLik1LR0T0lDgQOAJ4B+EbEiVa0E+qXjQcArJZfVpVi5eF0L8bK8EGlmhdLYhqXIiJgGTCvXRtLfAP8XODci3iqddo6IkDp3u4pH2mZWKO21EAkgaVuaE/bdEfGzFF6VpjZIP1eneD0wpOTywSlWLj64hXhZTtpmVijRhn/lpJ0c04GlEfHDkqq5wMYdIOOAB0riY9MukhHAm2kaZR4wUlLvtAA5EpiX6t6SNCK91tiSe22Rp0fMrFDa8YnII4HTgOclPZNiFwGTgTmSxgPLgZNT3UPACUAtsA44HSAi1ki6HFiU2l0WEWvS8ZnAHUBP4OFUyvJIu4PcOu06/qfuWZ55esH76s479xs0rK9np516t3ClFU1Lvwu9e/fikYfuZemSx3nkoXvp1WvHv7rm4IP24y/rlvOlL53Y2d3NXntt+YuIxyNCEbFvROyfykMR8XpEHBsRwyLiuI0JOO0amRARe0TEJyNiccm9ZkTEnqncXhJfHBGfSNeclXaRlOWk3UFmzZrDiZ/76vvigwcP5LPHfZrly+tauMqKqKXfhYnnT+DRxx7n7/b5FI8+9jgTz5+wqa5bt25c9f3vMn/+v3d2Vwuhvbb8dVVO2h3kN48/wZq1b7wvft21/4cLLrqSCv6gWkG09Lvw+c8fz6w7fwrArDt/yhe+8N4zFWdN+Do/u/8XrH719U7tZ1E0EBWXHDlpd6LPf34k9fUreO65F6vdFauyfrv0ZeXK5k0HK1eupt8ufQEYOLA/J40ZxY9vmVXN7mWtvRYiu6oPvBAp6fTSuZnN6mpofowTdd+Rbt22/6AvUxg9e27HhRPPZtQJX6l2V6wL2vg/rx9edykXXvR9/09sK/ijWbfsUqDFpF26YX2bHoP82wfsscdQhg79GL9bPB+AwYMHsOiJeRx+5ImsWvVqlXtnnW3V6tfo338XVq5cTf/+u2yaCjnowH25+66bAOjbtw+jRx1DQ0MDc+fOq2Z3s5LrCLpSZZO2pOe2VMV7j25aBV544T8ZOHi/Tee1v1/IYYeP5vXX11axV1YtD/78l4w97R+4+pobGXvaP/Dznzcn5WF7Hb6pzfTbrucXD/3KCbuNPuwj7X40f9jJ5plFwG87pEcFcdedN/KZTx9O3759+OOyxVx62bXcfsfsanfLqqCl34UfXHMjs+/5Mad/7VT++7/rOOUr/1TtbhZGY8GnllRu7kzSdOD2iHi8hbp7IqLVCVpPj5hZpRrW17f6edKt+cquX6w459yz/P6tfr3OVnakHRHjy9R5Rc3MupwP9Zy2mVluPuxz2mZmWangG2my5qRtZoXi6REzs4wUffeIk7aZFYqnR8zMMuKFSDOzjHhO28wsI54eMTPLSNE/IdFJ28wKpdEjbTOzfHh6xMwsI54eMTPLiEfaZmYZKfqWP3+xr5kVSmNExaU1kmZIWi3phZJYH0nzJb2cfvZOcUmaKqlW0nOSDiy5Zlxq/7KkcSXxgyQ9n66ZKqnVz/d20jazQmkiKi4VuAMYtVnsAmBBRAwDFqRzgNHAsFRqgJuhOckDk4DDgEOBSRsTfWpzRsl1m7/W+zhpm1mhtGfSjohfA2s2C48BZqbjmcBJJfFZ0Wwh0EvSAJq/snF+RKyJiLXAfGBUqtshIhZG8+rprJJ7bZGTtpkVSkRUXCTVSFpcUmoqeIl+EbEiHa/kvS85HwS8UtKuLsXKxetaiJflhUgzK5S27B6JiGnAtA/6WhERkjp15dMjbTMrlGjDvw9oVZraIP1cneL1wJCSdoNTrFx8cAvxspy0zaxQGqOp4vIBzQU27gAZBzxQEh+bdpGMAN5M0yjzgJGSeqcFyJHAvFT3lqQRadfI2JJ7bZGnR8ysUNrziUhJ9wJHA30l1dG8C2QyMEfSeGA5cHJq/hBwAlALrANOT/1ZI+lyYFFqd1lEbFzcPJPmHSo9gYdTKd+njn7kc5seg4q9093M2k3D+vpW9ym3Zr/+R1Scc55d+dutfr3O5pG2mRVK0Z+IdNI2s0Jp8gdGmZnlwyNtM7OMbMWukCw4aZtZoXh6xMwsI54eMTPLiEfaZmYZ8UjbzCwjjdFY7S50KCdtMysUf7GvmVlG/MW+ZmYZ8UjbzCwj3j1iZpYR7x4xM8uIH2M3M8uI57TNzDLiOW0zs4x4pG1mlhHv0zYzy4hH2mZmGfHuETOzjHgh0swsI54eMTPLiJ+INDPLiEfaZmYZKfqctor+V6krkVQTEdOq3Q/rWvx7YW3Rrdod+JCpqXYHrEvy74VVzEnbzCwjTtpmZhlx0u5cnre0lvj3wirmhUgzs4x4pG1mlhEnbTOzjDhpdxJJoyS9JKlW0gXV7o9Vn6QZklZLeqHafbF8OGl3AkndgRuB0cBw4FRJw6vbK+sC7gBGVbsTlhcn7c5xKFAbEcsiYj0wGxhT5T5ZlUXEr4E11e6H5cVJu3MMAl4pOa9LMTOzNnHSNjPLiJN256gHhpScD04xM7M2cdLuHIuAYZJ2k9QDOAWYW+U+mVmGnLQ7QUQ0AGcB84ClwJyIWFLdXlm1SboX+A9gL0l1ksZXu0/W9fkxdjOzjHikbWaWESdtM7OMOGmbmWXESdvMLCNO2mZmGXHSNjPLiJO2mVlG/j89jDrkUgA10QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83jxRr5VWjjg",
        "outputId": "a7fdb3eb-4f8d-4c73-d2d4-ecc7c3ef5d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "X.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.043949</td>\n",
              "      <td>0.318555</td>\n",
              "      <td>1.045810</td>\n",
              "      <td>2.805989</td>\n",
              "      <td>-0.561113</td>\n",
              "      <td>-0.367956</td>\n",
              "      <td>0.032736</td>\n",
              "      <td>-0.042333</td>\n",
              "      <td>-0.322674</td>\n",
              "      <td>0.499167</td>\n",
              "      <td>-0.572665</td>\n",
              "      <td>0.346009</td>\n",
              "      <td>-0.047407</td>\n",
              "      <td>-0.098964</td>\n",
              "      <td>-0.663284</td>\n",
              "      <td>0.181411</td>\n",
              "      <td>-0.124345</td>\n",
              "      <td>-0.790453</td>\n",
              "      <td>-0.720944</td>\n",
              "      <td>-0.084556</td>\n",
              "      <td>-0.240105</td>\n",
              "      <td>-0.680315</td>\n",
              "      <td>0.085328</td>\n",
              "      <td>0.684812</td>\n",
              "      <td>0.318620</td>\n",
              "      <td>-0.204963</td>\n",
              "      <td>0.001662</td>\n",
              "      <td>0.037894</td>\n",
              "      <td>-0.156600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.665159</td>\n",
              "      <td>0.808440</td>\n",
              "      <td>1.805627</td>\n",
              "      <td>1.903416</td>\n",
              "      <td>-0.821627</td>\n",
              "      <td>0.934790</td>\n",
              "      <td>-0.824802</td>\n",
              "      <td>0.975890</td>\n",
              "      <td>1.747469</td>\n",
              "      <td>-0.658751</td>\n",
              "      <td>1.281502</td>\n",
              "      <td>-1.430087</td>\n",
              "      <td>0.372028</td>\n",
              "      <td>1.403024</td>\n",
              "      <td>-2.739413</td>\n",
              "      <td>-1.331766</td>\n",
              "      <td>1.964590</td>\n",
              "      <td>-0.205639</td>\n",
              "      <td>1.325588</td>\n",
              "      <td>-0.373759</td>\n",
              "      <td>-0.335332</td>\n",
              "      <td>-0.510994</td>\n",
              "      <td>0.035839</td>\n",
              "      <td>0.147565</td>\n",
              "      <td>-0.529358</td>\n",
              "      <td>-0.566950</td>\n",
              "      <td>-0.595998</td>\n",
              "      <td>-0.220086</td>\n",
              "      <td>-0.288523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.324096</td>\n",
              "      <td>0.601836</td>\n",
              "      <td>0.865329</td>\n",
              "      <td>-2.138000</td>\n",
              "      <td>0.294663</td>\n",
              "      <td>-1.251553</td>\n",
              "      <td>1.072114</td>\n",
              "      <td>-0.334896</td>\n",
              "      <td>1.071268</td>\n",
              "      <td>-1.109522</td>\n",
              "      <td>-1.016020</td>\n",
              "      <td>-0.654945</td>\n",
              "      <td>-1.473470</td>\n",
              "      <td>0.317345</td>\n",
              "      <td>1.067491</td>\n",
              "      <td>-0.372642</td>\n",
              "      <td>-0.674725</td>\n",
              "      <td>0.369841</td>\n",
              "      <td>0.095583</td>\n",
              "      <td>-0.039868</td>\n",
              "      <td>0.012220</td>\n",
              "      <td>0.352856</td>\n",
              "      <td>-0.341505</td>\n",
              "      <td>-0.145791</td>\n",
              "      <td>0.094194</td>\n",
              "      <td>-0.804026</td>\n",
              "      <td>0.229428</td>\n",
              "      <td>-0.021623</td>\n",
              "      <td>-0.352771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.258270</td>\n",
              "      <td>1.217501</td>\n",
              "      <td>-0.585348</td>\n",
              "      <td>-0.875347</td>\n",
              "      <td>1.222481</td>\n",
              "      <td>-0.311027</td>\n",
              "      <td>1.073860</td>\n",
              "      <td>-0.161408</td>\n",
              "      <td>0.200665</td>\n",
              "      <td>0.154307</td>\n",
              "      <td>0.882673</td>\n",
              "      <td>0.547890</td>\n",
              "      <td>0.269484</td>\n",
              "      <td>-1.253302</td>\n",
              "      <td>-0.883963</td>\n",
              "      <td>0.495221</td>\n",
              "      <td>-0.153212</td>\n",
              "      <td>0.296710</td>\n",
              "      <td>0.136148</td>\n",
              "      <td>0.382305</td>\n",
              "      <td>-0.424626</td>\n",
              "      <td>-0.781158</td>\n",
              "      <td>0.019316</td>\n",
              "      <td>0.178614</td>\n",
              "      <td>-0.315616</td>\n",
              "      <td>0.096665</td>\n",
              "      <td>0.269740</td>\n",
              "      <td>-0.020635</td>\n",
              "      <td>-0.313351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.142162</td>\n",
              "      <td>-0.494988</td>\n",
              "      <td>-1.936511</td>\n",
              "      <td>-0.818288</td>\n",
              "      <td>-0.025213</td>\n",
              "      <td>-1.027245</td>\n",
              "      <td>-0.151627</td>\n",
              "      <td>-0.305750</td>\n",
              "      <td>-0.869482</td>\n",
              "      <td>0.428729</td>\n",
              "      <td>1.136666</td>\n",
              "      <td>0.273476</td>\n",
              "      <td>0.697123</td>\n",
              "      <td>-1.222134</td>\n",
              "      <td>-0.938820</td>\n",
              "      <td>1.298149</td>\n",
              "      <td>0.912921</td>\n",
              "      <td>-0.793721</td>\n",
              "      <td>1.064984</td>\n",
              "      <td>0.106592</td>\n",
              "      <td>0.010115</td>\n",
              "      <td>0.021722</td>\n",
              "      <td>0.079463</td>\n",
              "      <td>-0.480899</td>\n",
              "      <td>0.023846</td>\n",
              "      <td>-0.279076</td>\n",
              "      <td>-0.030121</td>\n",
              "      <td>-0.043888</td>\n",
              "      <td>-0.195737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-6.341667</td>\n",
              "      <td>1.192815</td>\n",
              "      <td>-3.686579</td>\n",
              "      <td>-2.754006</td>\n",
              "      <td>-5.007164</td>\n",
              "      <td>3.988118</td>\n",
              "      <td>1.693606</td>\n",
              "      <td>-13.577522</td>\n",
              "      <td>2.557965</td>\n",
              "      <td>-0.931438</td>\n",
              "      <td>-1.189333</td>\n",
              "      <td>0.762725</td>\n",
              "      <td>-0.517857</td>\n",
              "      <td>0.067279</td>\n",
              "      <td>-0.099498</td>\n",
              "      <td>0.837768</td>\n",
              "      <td>-0.156679</td>\n",
              "      <td>-0.402600</td>\n",
              "      <td>-0.238742</td>\n",
              "      <td>-7.171342</td>\n",
              "      <td>9.779321</td>\n",
              "      <td>-3.660198</td>\n",
              "      <td>-1.507217</td>\n",
              "      <td>-0.179185</td>\n",
              "      <td>0.002092</td>\n",
              "      <td>-0.019168</td>\n",
              "      <td>3.611243</td>\n",
              "      <td>-0.894494</td>\n",
              "      <td>5.435550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.463928</td>\n",
              "      <td>0.204745</td>\n",
              "      <td>2.795734</td>\n",
              "      <td>1.546200</td>\n",
              "      <td>-0.720374</td>\n",
              "      <td>1.108212</td>\n",
              "      <td>-0.581133</td>\n",
              "      <td>0.544075</td>\n",
              "      <td>1.099843</td>\n",
              "      <td>-0.653523</td>\n",
              "      <td>-1.420838</td>\n",
              "      <td>0.427211</td>\n",
              "      <td>-0.436201</td>\n",
              "      <td>-1.055012</td>\n",
              "      <td>-1.225903</td>\n",
              "      <td>-1.179268</td>\n",
              "      <td>0.846734</td>\n",
              "      <td>-0.374502</td>\n",
              "      <td>0.623190</td>\n",
              "      <td>-0.052393</td>\n",
              "      <td>0.094431</td>\n",
              "      <td>0.788161</td>\n",
              "      <td>-0.100344</td>\n",
              "      <td>0.126248</td>\n",
              "      <td>-0.461584</td>\n",
              "      <td>-0.200985</td>\n",
              "      <td>0.280168</td>\n",
              "      <td>0.180046</td>\n",
              "      <td>-0.312061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1.394619</td>\n",
              "      <td>0.554224</td>\n",
              "      <td>-0.617805</td>\n",
              "      <td>-0.313706</td>\n",
              "      <td>2.157413</td>\n",
              "      <td>3.537534</td>\n",
              "      <td>-0.567305</td>\n",
              "      <td>1.380888</td>\n",
              "      <td>-0.112291</td>\n",
              "      <td>0.107269</td>\n",
              "      <td>-0.637728</td>\n",
              "      <td>0.099510</td>\n",
              "      <td>-0.062533</td>\n",
              "      <td>0.254824</td>\n",
              "      <td>0.318805</td>\n",
              "      <td>-0.335245</td>\n",
              "      <td>0.052870</td>\n",
              "      <td>0.048783</td>\n",
              "      <td>1.654699</td>\n",
              "      <td>0.336595</td>\n",
              "      <td>-0.196300</td>\n",
              "      <td>-0.546172</td>\n",
              "      <td>-0.124758</td>\n",
              "      <td>1.046780</td>\n",
              "      <td>-0.089474</td>\n",
              "      <td>0.363674</td>\n",
              "      <td>0.215692</td>\n",
              "      <td>0.225698</td>\n",
              "      <td>-0.259744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-2.121785</td>\n",
              "      <td>-1.518485</td>\n",
              "      <td>2.667942</td>\n",
              "      <td>-2.458839</td>\n",
              "      <td>-1.414952</td>\n",
              "      <td>0.807935</td>\n",
              "      <td>-1.870160</td>\n",
              "      <td>0.516477</td>\n",
              "      <td>-0.553359</td>\n",
              "      <td>0.645799</td>\n",
              "      <td>-0.514941</td>\n",
              "      <td>0.618199</td>\n",
              "      <td>1.461221</td>\n",
              "      <td>-2.116306</td>\n",
              "      <td>-3.244732</td>\n",
              "      <td>-0.345286</td>\n",
              "      <td>0.368870</td>\n",
              "      <td>0.699637</td>\n",
              "      <td>0.031145</td>\n",
              "      <td>-0.684797</td>\n",
              "      <td>-0.079591</td>\n",
              "      <td>0.659885</td>\n",
              "      <td>-1.000976</td>\n",
              "      <td>-0.212991</td>\n",
              "      <td>0.420722</td>\n",
              "      <td>-0.019645</td>\n",
              "      <td>-0.482792</td>\n",
              "      <td>-0.407780</td>\n",
              "      <td>-0.243944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-2.254788</td>\n",
              "      <td>-0.955967</td>\n",
              "      <td>0.590596</td>\n",
              "      <td>0.114744</td>\n",
              "      <td>-5.876531</td>\n",
              "      <td>3.364858</td>\n",
              "      <td>6.365904</td>\n",
              "      <td>-1.252960</td>\n",
              "      <td>0.243849</td>\n",
              "      <td>-1.637022</td>\n",
              "      <td>-0.626434</td>\n",
              "      <td>0.105457</td>\n",
              "      <td>0.859541</td>\n",
              "      <td>-1.306715</td>\n",
              "      <td>-0.724693</td>\n",
              "      <td>0.553933</td>\n",
              "      <td>-0.590110</td>\n",
              "      <td>-0.471491</td>\n",
              "      <td>-0.291435</td>\n",
              "      <td>-0.772127</td>\n",
              "      <td>-0.601908</td>\n",
              "      <td>-0.092918</td>\n",
              "      <td>0.517855</td>\n",
              "      <td>0.534361</td>\n",
              "      <td>0.494493</td>\n",
              "      <td>0.301414</td>\n",
              "      <td>0.507071</td>\n",
              "      <td>-0.560888</td>\n",
              "      <td>5.104634</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         V1        V2        V3  ...       V27       V28    Amount\n",
              "0  1.043949  0.318555  1.045810  ...  0.001662  0.037894 -0.156600\n",
              "1 -1.665159  0.808440  1.805627  ... -0.595998 -0.220086 -0.288523\n",
              "2 -0.324096  0.601836  0.865329  ...  0.229428 -0.021623 -0.352771\n",
              "3 -0.258270  1.217501 -0.585348  ...  0.269740 -0.020635 -0.313351\n",
              "4  2.142162 -0.494988 -1.936511  ... -0.030121 -0.043888 -0.195737\n",
              "5 -6.341667  1.192815 -3.686579  ...  3.611243 -0.894494  5.435550\n",
              "6 -0.463928  0.204745  2.795734  ...  0.280168  0.180046 -0.312061\n",
              "7 -1.394619  0.554224 -0.617805  ...  0.215692  0.225698 -0.259744\n",
              "8 -2.121785 -1.518485  2.667942  ... -0.482792 -0.407780 -0.243944\n",
              "9 -2.254788 -0.955967  0.590596  ...  0.507071 -0.560888  5.104634\n",
              "\n",
              "[10 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKzpBrF9I00U"
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import numpy as np\n",
        "\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12,input_dim=29, activation='relu')) \n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgb8mwFZe_sp"
      },
      "source": [
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "model = KerasClassifier(build_fn=create_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evQypnUYaJWC",
        "outputId": "4788bced-a656-4fbe-ad00-b8ab77790444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\t#Random seed\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define the grid search parameters\n",
        "\n",
        "batch_size=[1000,5000]#add 50,100 etc\n",
        "epochs=[1,10]#add 50,100 etc\n",
        "optimizer = ['SGD','RMSprop','Adagrad','Adam']\n",
        "#model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "param_grid = dict(batch_size=batch_size,epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X,y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.5606 - accuracy: 0.7271\n",
            "Epoch 2/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.1653 - accuracy: 0.9970\n",
            "Epoch 3/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0654 - accuracy: 0.9990\n",
            "Epoch 4/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0336 - accuracy: 0.9991\n",
            "Epoch 5/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0214 - accuracy: 0.9992\n",
            "Epoch 6/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0154 - accuracy: 0.9992\n",
            "Epoch 7/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0119 - accuracy: 0.9992\n",
            "Epoch 8/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0097 - accuracy: 0.9992\n",
            "Epoch 9/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0081 - accuracy: 0.9993\n",
            "Epoch 10/10\n",
            "228/228 [==============================] - 1s 3ms/step - loss: 0.0070 - accuracy: 0.9993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ZfHQhta_8g",
        "outputId": "7876261d-728b-4276-c23c-79364d8bc774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.999307 using {'batch_size': 1000, 'epochs': 10}\n",
            "0.982453 (0.016113) with: {'batch_size': 1000, 'epochs': 1}\n",
            "0.999307 (0.000030) with: {'batch_size': 1000, 'epochs': 10}\n",
            "0.688806 (0.168766) with: {'batch_size': 5000, 'epochs': 1}\n",
            "0.998301 (0.001281) with: {'batch_size': 5000, 'epochs': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xomgs0-Hk3I8",
        "outputId": "24f94e88-4d03-4c72-dae1-527a2cc31ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "The batch size of 1000 and 10 epochs has the best accuracy score of 99.99%.\n",
        "For now I have kept epoch very small because it was taking time. We should test higher values also."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'summary'"
          ]
        }
      ]
    }
  ]
}